# -*- coding: utf-8 -*-
"""sparkstream.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/182-flaHh6u6z_F45uLagtmpP6E5WRRkb
"""

import os
import sys
from kafka import KafkaProducer
from flume import Flume

# configuring Kafka
KAFKA_BOOTSTRAP_SERVERS = 'localhost:9092'
KAFKA_TOPIC = 'raw_data'

# configuring Flume
FLUME_AGENT_HOST = 'localhost'
FLUME_AGENT_PORT = 41414
FLUME_CHANNEL = 'emory_channel'
FLUME_SINK = 'hdfs_sink'

# Kafka producer
producer = KafkaProducer(bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS)

# Flume agent
flume_agent = Flume(FLUME_AGENT_HOST, FLUME_AGENT_PORT)

# function to ingest sample data from social media
def ingest_social_media_data():
    # Simulate social media data ingestion (e.g. Twitter API)
    data = [{'text': 'Hello World!', 'user': 'john'}, {'text': 'This is a test', 'user': 'jane'}]
    for message in data:
        producer.send(KAFKA_TOPIC, value=message)

# function to ingest sample data from IoT devices
def ingest_iot_data():
    # Simulate IoT data ingestion (e.g. MQTT broker)
    data = [{'device_id': 'device1', 'temperature': 25.0}, {'device_id': 'device2', 'temperature': 30.0}]
    for message in data:
        producer.send(KAFKA_TOPIC, value=message)

# function to ingest sample data from web logs
def ingest_web_log_data():
    # Simulate web log data ingestion (e.g. Apache HTTP Server logs)
    data = [{'ip_address': '192.168.1.1', 'equest_method': 'GET', 'esponse_code': 200}, {'ip_address': '192.168.1.2', 'equest_method': 'POST', 'esponse_code': 404}]
    for message in data:
        producer.send(KAFKA_TOPIC, value=message)

# Flume event handler to handle Kafka messages
def handle_kafka_message(message):
    event = flume_agent.create_event(message.value)
    flume_agent.send_event(event, FLUME_CHANNEL)

# Subscribe to the Kafka topic and start consuming messages
consumer = KafkaConsumer(KAFKA_TOPIC, bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS)
consumer.subscribe([KAFKA_TOPIC])

for message in consumer:
    handle_kafka_message(message)

flume_agent.start()

ingest_social_media_data()
ingest_iot_data()
ingest_web_log_data()

flume_agent.stop()

from pyspark.sql import SparkSession
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.clustering import KMeans
from pyspark.ml.regression import LinearRegression
from pyspark.ml.classification import LogisticRegression
from pyspark.sql.functions import *

# Creating SparkSession
spark = SparkSession.builder.appName("DataProcessing").getOrCreate()

# StreamingContext with batch interval of 10 sec
ssc = StreamingContext(spark.sparkContext, 10)

# Reading data from Kafka
kafka_df = spark.read.format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "raw_data") \
    .load()

# Converting Kafka data to a DataFrame
data_df = kafka_df.selectExpr("CAST(value AS STRING) as json").select(from_json("json", "MAP<STRING,STRING>").alias("data"))

# Extracting relevant columns from DataFrame
processed_df = data_df.select("data.device_id", "data.temperature", "data.humidity", "data.pressure")

# Performing data transformations and aggregation
processed_df = processed_df.withColumnRenamed("device_id", "id").withColumnRenamed("temperature", "temp").withColumnRenamed("humidity", "hum").withColumnRenamed("pressure", "pres")
processed_df = processed_df.withColumn("timestamp", current_timestamp())
processed_df = processed_df.groupBy("id", window("timestamp", "10 minutes")).agg(mean("temp").alias("avg_temp"), mean("hum").alias("avg_hum"), mean("pres").alias("avg_pres"))

# Clustering
assembler = VectorAssembler(inputCols=["avg_temp", "avg_hum", "avg_pres"], outputCol="features")
clustering_df = assembler.transform(processed_df)
kmeans = KMeans().setK(2).setSeed(1)
model = kmeans.fit(clustering_df.select("features"))
clustered_df = model.transform(clustering_df)

# Classification
assembler = VectorAssembler(inputCols=["avg_temp", "avg_hum", "avg_pres"], outputCol="features")
classification_df = assembler.transform(processed_df)
label_df = classification_df.withColumn("label", when(col("avg_temp") > 30, 1).otherwise(0))
lr = LogisticRegression(featuresCol="features", labelCol="label")
model = lr.fit(label_df)
predicted_df = model.transform(classification_df)

# Regression
assembler = VectorAssembler(inputCols=["avg_temp", "avg_hum"], outputCol="features")
regression_df = assembler.transform(processed_df)
lr = LinearRegression(featuresCol="features", labelCol="avg_pres")
model = lr.fit(regression_df)
predicted_df = model.transform(regression_df)

# Writing processed data to HDFS
processed_df.write.format("parquet").mode("append").save("/user/hadoop/processed_data")

# Start streaming context
ssc.start()
ssc.awaitTermination()

from pyspark.sql import SparkSession
from pyspark.sql.types import *

# Creating SparkSession
spark = SparkSession.builder.appName("DataStorage").getOrCreate()

# Writing raw data to HDFS
raw_data_df.write.format("parquet").mode("append").save("/user/hadoop/raw_data")

# Writing processed data to HDFS
processed_data_df.write.format("parquet").mode("append").save("/user/hadoop/processed_data")

# Creating Hive table for the raw data
spark.sql("CREATE TABLE IF NOT EXISTS raw_data (device_id STRING, temperature DOUBLE, humidity DOUBLE, pressure DOUBLE) STORED AS PARQUET LOCATION '/user/hadoop/raw_data'")

# Creating Hive table for the processed data
spark.sql("CREATE TABLE IF NOT EXISTS processed_data (device_id STRING, avg_temp DOUBLE, avg_hum DOUBLE, avg_pres DOUBLE) STORED AS PARQUET LOCATION '/user/hadoop/processed_data'")

spark.sql("SELECT * FROM raw_data WHERE temperature > 30").show()
spark.sql("SELECT device_id, AVG(avg_temp) AS avg_temp FROM processed_data GROUP BY device_id").show()

# Creating Hive view from processed data
spark.sql("CREATE VIEW IF NOT EXISTS processed_data_view AS SELECT * FROM processed_data")

spark.sql("SELECT * FROM processed_data_view WHERE avg_temp > 25").show()
spark.sql("SELECT device_id, COUNT(*) AS count FROM processed_data_view GROUP BY device_id").show()
spark.stop()

import pandas as pd
import tableau_api_lib as tal

# Loading processed data from HDFS
processed_data_df = spark.read.parquet("/user/hadoop/processed_data")

# Converting Spark DataFrame to a Pandas DataFrame
processed_data_pd = processed_data_df.toPandas()

# Creating Tableau data extract
data_extract = tal.DataExtract(processed_data_pd)

# Creating Tableau workbook
workbook = tal.Workbook("Real-Time Streaming Data Analysis")

# Creating dashboard
dashboard = workbook.dashboards.add("Real-Time Insights")

# Map Visualization
map_visualization = dashboard.visualizations.add("Map")
map_visualization.data.extract = data_extract
map_visualization.fields.latitude = "device_latitude"
map_visualization.fields.longitude = "device_longitude"
map_visualization.fields.size = "avg_temp"

# Bar Chart
bar_chart_visualization = dashboard.visualizations.add("Bar Chart")
bar_chart_visualization.data.extract = data_extract
bar_chart_visualization.fields.column = "device_id"
bar_chart_visualization.fields.size = "avg_temp"

# Line Chart
line_chart_visualization = dashboard.visualizations.add("Line Chart")
line_chart_visualization.data.extract = data_extract
line_chart_visualization.fields.column = "timestamp"
line_chart_visualization.fields.line = "avg_temp"

# Publishing the workbook
workbook.publish("https://your-tableau-server.com", "your_username", "your_password")

workbook.open()